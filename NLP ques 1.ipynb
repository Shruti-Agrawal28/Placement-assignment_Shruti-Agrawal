{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c40e590",
   "metadata": {},
   "source": [
    "Take any YouTube videos link and your task is to extract the comments from\n",
    "that videos and store it in a csv file and then you need define what is most\n",
    "demanding topic in that videos comment section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3e9ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from youtube_comment_scraper_python import *\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26be33c7",
   "metadata": {},
   "source": [
    "### Extracting comments from youtube using youtube_comment_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48837c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DataKund...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|████████████████████████████████████████████████████████████████████| 200.0/200 [01:08<00:00,  2.94it/s]\n"
     ]
    }
   ],
   "source": [
    "youtube.open(\"https://www.youtube.com/watch?v=fM4qTMfCoak&feature=youtu.be\")\n",
    "response=youtube.video_comments()\n",
    "data=response['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8248746",
   "metadata": {},
   "source": [
    "### Saving the data into dataframe and then into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c5a87db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He is that teacher which our education system ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nice, I got it what is NLP. thank you so much ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sir can u make one for computer vision?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i think even glove is a good algo for nlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Very nice explanation. Sir i have completed  o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment\n",
       "0  He is that teacher which our education system ...\n",
       "1  Nice, I got it what is NLP. thank you so much ...\n",
       "2            Sir can u make one for computer vision?\n",
       "3          i think even glove is a good algo for nlp\n",
       "4  Very nice explanation. Sir i have completed  o..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd   \n",
    "df = pd.DataFrame(data, columns=['Comment'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "652adc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"youtube_comments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80101eb0",
   "metadata": {},
   "source": [
    "### Cleaning data using stopwords and then defining the most demanding topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4fc7441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most demanding topic in the comments section is 1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess the comments\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(comment):\n",
    "    # Tokenize the comment\n",
    "    tokens = word_tokenize(comment.lower())\n",
    "\n",
    "    # Remove stop words and non-alphabetic tokens\n",
    "    filtered_tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "    # Perform lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Remove short tokens\n",
    "    long_tokens = [token for token in lemmatized_tokens if len(token) > 2]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_comment = ' '.join(long_tokens)\n",
    "\n",
    "    return preprocessed_comment\n",
    "\n",
    "df['preprocessed_comments'] = df['Comment'].apply(preprocess_text)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed comments\n",
    "tfidf_matrix = vectorizer.fit_transform(df['preprocessed_comments'])\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Perform Latent Dirichlet Allocation (LDA) topic modeling\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda_model.fit(tfidf_matrix)\n",
    "\n",
    "# Get the most probable topic for each comment\n",
    "topic_probabilities = lda_model.transform(tfidf_matrix)\n",
    "dominant_topics = topic_probabilities.argmax(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Map topic numbers to labels\n",
    "#dominant_topics = [topic_labels[topic] for topic in dominant_topics]\n",
    "\n",
    "# Add the dominant topics to the dataframe\n",
    "df['dominant_topic'] = dominant_topics\n",
    "\n",
    "# Count the occurrences of each topic\n",
    "topic_counts = df['dominant_topic'].value_counts()\n",
    "\n",
    "# Get the most demanding topic\n",
    "most_demanding_topic = topic_counts.idxmax()\n",
    "\n",
    "print(f\"The most demanding topic in the comments section is {most_demanding_topic}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc73179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i think even glove is a good algo for nlp\n",
      "And, could you also put up some material on how Hidden Markov Models are used in NLP? have studied them way back in 2011 during my Master's degree in the pre Deep Learning era. But don't have much practical exposure to NLP? And does acoustic model for phonemes recognition come more under speech Recognition? Could you also provide a short description on that?\n",
      "Krish where are spark sessions??\n",
      "Hi @Krish, could u please make video on ..what is most challenging task u have face in during ur project ..and how Do resolve it ..this is one the question I have faced during the interview ..eagerly waiting more videos on interview playlist\n"
     ]
    }
   ],
   "source": [
    "for i in df.dominant_topic.index:\n",
    "    if df.dominant_topic[i] == 1:\n",
    "        print(df[\"Comment\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8889be72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
